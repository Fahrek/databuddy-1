# release setup
free=None #'#FreeUkraine #SaveUkraine #StopRussia #PutinKhuilo #CrimeaIsUkraine'
bin='32'
version='0.3.5'
ts='2016/06/03 10:59:45'
for_db=['MYSQL']
from_to_dbs=['MYSQL', 'CSV']
ff=['CSV']
release=True
app_author='Alex Buzunov'
apptitle='QueryCopy'
appn='qc'
appex='exe'
appname='qc32.exe'
tests=[[['MYSQL_Partition_Limit22', 'MYSQL_QueryFile_Limit100', 'MYSQL_QueryDir_Limit333', 'MYSQL_ShardedPartition', 'MYSQL_Subpartition', 'MYSQL_ShardedSubpartition', 'MYSQL_TimezoneQueryFile', 'MYSQL_Table_Limit1000', 'MYSQL_QueryDir', 'MYSQL_Table_KeepSpoolFile', 'MYSQL_Partition', 'MYSQL_Table', 'MYSQL_ShardedQuery', 'MYSQL_Subpartition_Limit33', 'MYSQL_ShardedTable', 'MYSQL_QueryFile'], ['MYSQL_Table']], [['MYSQL_Partition_Limit22', 'MYSQL_QueryFile_Limit100', 'MYSQL_QueryDir_Limit333', 'MYSQL_ShardedPartition', 'MYSQL_Subpartition', 'MYSQL_ShardedSubpartition', 'MYSQL_TimezoneQueryFile', 'MYSQL_Table_Limit1000', 'MYSQL_QueryDir', 'MYSQL_Table_KeepSpoolFile', 'MYSQL_Partition', 'MYSQL_Table', 'MYSQL_ShardedQuery', 'MYSQL_Subpartition_Limit33', 'MYSQL_ShardedTable', 'MYSQL_QueryFile'], ['CSV_Dir', 'CSV_Default', 'CSV_File']], [['CSV_Dirs', 'CSV_ShardedFile', 'CSV_Dirs_Limit10', 'CSV_File_TableNamedFile', 'CSV_File_Limit10', 'CSV_ShardedFileSkip1', 'CSV_DateFile', 'CSV_TimestampFile', 'CSV_TimezoneFile', 'CSV_Files_TableNamedFile', 'CSV_Dir', 'CSV_ShardedDir_Limit10', 'CSV_FileSkip1', 'CSV_ShardedDirSkip1', 'CSV_File', 'CSV_DateFiles', 'CSV_MongoFile', 'CSV_ShardedFile_Limit10', 'CSV_ShardedDir', 'CSV_DirsSkip1'], ['MYSQL_Table']]]
_to='-'
ucases=None
import __builtin__
__builtin__.for_db = for_db
__builtin__.from_to_dbs=from_to_dbs
__builtin__.free=free
__builtin__.apptitle=apptitle
__builtin__.version=version
__builtin__.bin=bin
__builtin__.appname=appname
__builtin__.release=release
__builtin__._to=_to
import test.v101.build_include.include_dm_for_db as inc
def get_ucases():
	return ucases
def show_default_help():
	inc.show_default_help(for_db, from_to_dbs,appname,dbs,ff)
def supported(x):
	return inc.supported(x, for_db, from_to_dbs)
def getAppTitle():
	return inc.getAppTitle(for_db, apptitle,dbs)
def getExeTitle():
	return inc.getExeTitle(apptitle, for_db, bin, dbs)	
def show_help(cvarg,copy_vector,params):
	inc.show_help(for_db,from_to_dbs,dbs,cvarg,copy_vector,params,ucases)

ucases={'MySQL_to_CSV': {'MYSQL_ShardedPartition_to_CSV_Default': ('MYSQL', 'CSV', ('', 'Break input sharded partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Extract MySQL sharded partition into CSV Default location.'), 'Use case name: MYSQL_ShardedPartition_to_CSV_Default\nDescription:  Break input sharded partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  Extract MySQL sharded partition into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_721000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Table_to_CSV_Dir': ('MYSQL', 'CSV', ('', '', 'Extract MySQL table into CSV Dir location.'), 'Use case name: MYSQL_Table_to_CSV_Dir\nDescription:  Extract MySQL table into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_861000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_QueryDir_to_CSV_Default': ('MYSQL', 'CSV', ('Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  ', '', 'Extract MySQL query results into CSV Default location.'), 'Use case name: MYSQL_QueryDir_to_CSV_Default\nDescription:  Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  Extract MySQL query results into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -Q[--query_sql_dir] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_614000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -Q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_QueryFile_to_CSV_Dir': ('MYSQL', 'CSV', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Extract MySQL query results into CSV Dir location.'), 'Use case name: MYSQL_QueryFile_to_CSV_Dir\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Extract MySQL query results into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_545000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_QueryFile_to_CSV_Default': ('MYSQL', 'CSV', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Extract MySQL query results into CSV Default location.'), 'Use case name: MYSQL_QueryFile_to_CSV_Default\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Extract MySQL query results into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_608000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_ShardedPartition_to_CSV_Dir': ('MYSQL', 'CSV', ('', 'Break input sharded partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Extract MySQL sharded partition into CSV Dir location.'), 'Use case name: MYSQL_ShardedPartition_to_CSV_Dir\nDescription:  Break input sharded partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  Extract MySQL sharded partition into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_659000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_Table_Limit1000_to_CSV_Default': ('MYSQL', 'CSV', ('', '', 'Extract only 1000 rows from MySQL table into CSV Default location.'), 'Use case name: MYSQL_Table_Limit1000_to_CSV_Default\nDescription:  Extract only 1000 rows from MySQL table into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 1000 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_412000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Partition_Limit22_to_CSV_File': ('MYSQL', 'CSV', ('', '', 'Extract only 22 rows from MySQL partition into CSV File location.'), 'Use case name: MYSQL_Partition_Limit22_to_CSV_File\nDescription:  Extract only 22 rows from MySQL partition into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 22 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_159000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_Partition_Limit22.csv'), 'MYSQL_QueryFile_Limit100_to_CSV_Default': ('MYSQL', 'CSV', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Extract only 100 rows from MySQL query results into CSV Default location.'), 'Use case name: MYSQL_QueryFile_Limit100_to_CSV_Default\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Extract only 100 rows from MySQL query results into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 100 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_321000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_QueryDir_Limit333_to_CSV_File': ('MYSQL', 'CSV', ('Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  ', '', 'Extract only 333 rows from MySQL query results into CSV File location.'), 'Use case name: MYSQL_QueryDir_Limit333_to_CSV_File\nDescription:  Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  Extract only 333 rows from MySQL query results into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -Q[--query_sql_dir] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 333 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_590000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -Q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_QueryDir_Limit333.csv'), 'MYSQL_ShardedQuery_to_CSV_Dir': ('MYSQL', 'CSV', ('', 'Break input query results into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Extract MySQL query results into CSV Dir location.'), 'Use case name: MYSQL_ShardedQuery_to_CSV_Dir\nDescription:  Break input query results into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  Extract MySQL query results into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_047000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_Partition_to_CSV_Default': ('MYSQL', 'CSV', ('', '', 'Extract MySQL partition into CSV Default location.'), 'Use case name: MYSQL_Partition_to_CSV_Default\nDescription:  Extract MySQL partition into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_743000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_QueryDir_Limit333_to_CSV_Default': ('MYSQL', 'CSV', ('Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  ', '', 'Extract only 333 rows from MySQL query results into CSV Default location.'), 'Use case name: MYSQL_QueryDir_Limit333_to_CSV_Default\nDescription:  Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  Extract only 333 rows from MySQL query results into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -Q[--query_sql_dir] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 333 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_521000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -Q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_TimezoneQueryFile_to_CSV_Default': ('MYSQL', 'CSV', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Extract MySQL query results into CSV Default location.'), 'Use case name: MYSQL_TimezoneQueryFile_to_CSV_Default\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Extract MySQL query results into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_160000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_ShardedSubpartition_to_CSV_Dir': ('MYSQL', 'CSV', ('', 'Break input sharded sub-partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Extract MySQL sharded sub-partition into CSV Dir location.'), 'Use case name: MYSQL_ShardedSubpartition_to_CSV_Dir\nDescription:  Break input sharded sub-partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  Extract MySQL sharded sub-partition into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_984000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_QueryDir_to_CSV_Dir': ('MYSQL', 'CSV', ('Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  ', '', 'Extract MySQL query results into CSV Dir location.'), 'Use case name: MYSQL_QueryDir_to_CSV_Dir\nDescription:  Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  Extract MySQL query results into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -Q[--query_sql_dir] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_544000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -Q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_Subpartition_to_CSV_Default': ('MYSQL', 'CSV', ('', '', 'Extract MySQL sub-partition into CSV Default location.'), 'Use case name: MYSQL_Subpartition_to_CSV_Default\nDescription:  Extract MySQL sub-partition into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_844000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Subpartition_to_CSV_File': ('MYSQL', 'CSV', ('', '', 'Extract MySQL sub-partition into CSV File location.'), 'Use case name: MYSQL_Subpartition_to_CSV_File\nDescription:  Extract MySQL sub-partition into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_911000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_Subpartition.csv'), 'MYSQL_QueryFile_Limit100_to_CSV_Dir': ('MYSQL', 'CSV', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Extract only 100 rows from MySQL query results into CSV Dir location.'), 'Use case name: MYSQL_QueryFile_Limit100_to_CSV_Dir\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Extract only 100 rows from MySQL query results into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 100 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_259000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_QueryFile_to_CSV_File': ('MYSQL', 'CSV', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Extract MySQL query results into CSV File location.'), 'Use case name: MYSQL_QueryFile_to_CSV_File\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Extract MySQL query results into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_676000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_QueryFile.csv'), 'MYSQL_QueryFile_Limit100_to_CSV_File': ('MYSQL', 'CSV', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Extract only 100 rows from MySQL query results into CSV File location.'), 'Use case name: MYSQL_QueryFile_Limit100_to_CSV_File\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Extract only 100 rows from MySQL query results into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 100 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_390000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_QueryFile_Limit100.csv'), 'MYSQL_QueryDir_Limit333_to_CSV_Dir': ('MYSQL', 'CSV', ('Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  ', '', 'Extract only 333 rows from MySQL query results into CSV Dir location.'), 'Use case name: MYSQL_QueryDir_Limit333_to_CSV_Dir\nDescription:  Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  Extract only 333 rows from MySQL query results into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -Q[--query_sql_dir] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 333 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_459000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -Q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_Subpartition_Limit33_to_CSV_Dir': ('MYSQL', 'CSV', ('', '', 'Extract only 33 rows from MySQL sub-partition into CSV Dir location.'), 'Use case name: MYSQL_Subpartition_Limit33_to_CSV_Dir\nDescription:  Extract only 33 rows from MySQL sub-partition into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 33 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_176000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_ShardedSubpartition_to_CSV_Default': ('MYSQL', 'CSV', ('', 'Break input sharded sub-partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Extract MySQL sharded sub-partition into CSV Default location.'), 'Use case name: MYSQL_ShardedSubpartition_to_CSV_Default\nDescription:  Break input sharded sub-partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  Extract MySQL sharded sub-partition into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_044000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Partition_to_CSV_Dir': ('MYSQL', 'CSV', ('', '', 'Extract MySQL partition into CSV Dir location.'), 'Use case name: MYSQL_Partition_to_CSV_Dir\nDescription:  Extract MySQL partition into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_676000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_ShardedQuery_to_CSV_Default': ('MYSQL', 'CSV', ('', 'Break input query results into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Extract MySQL query results into CSV Default location.'), 'Use case name: MYSQL_ShardedQuery_to_CSV_Default\nDescription:  Break input query results into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  Extract MySQL query results into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_125000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Table_Limit1000_to_CSV_File': ('MYSQL', 'CSV', ('', '', 'Extract only 1000 rows from MySQL table into CSV File location.'), 'Use case name: MYSQL_Table_Limit1000_to_CSV_File\nDescription:  Extract only 1000 rows from MySQL table into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 1000 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_470000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_Table_Limit1000.csv'), 'MYSQL_Partition_Limit22_to_CSV_Dir': ('MYSQL', 'CSV', ('', '', 'Extract only 22 rows from MySQL partition into CSV Dir location.'), 'Use case name: MYSQL_Partition_Limit22_to_CSV_Dir\nDescription:  Extract only 22 rows from MySQL partition into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 22 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_021000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_TimezoneQueryFile_to_CSV_File': ('MYSQL', 'CSV', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Extract MySQL query results into CSV File location.'), 'Use case name: MYSQL_TimezoneQueryFile_to_CSV_File\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Extract MySQL query results into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_276000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_TimezoneQueryFile.csv'), 'MYSQL_Table_Limit1000_to_CSV_Dir': ('MYSQL', 'CSV', ('', '', 'Extract only 1000 rows from MySQL table into CSV Dir location.'), 'Use case name: MYSQL_Table_Limit1000_to_CSV_Dir\nDescription:  Extract only 1000 rows from MySQL table into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 1000 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_345000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_Subpartition_Limit33_to_CSV_Default': ('MYSQL', 'CSV', ('', '', 'Extract only 33 rows from MySQL sub-partition into CSV Default location.'), 'Use case name: MYSQL_Subpartition_Limit33_to_CSV_Default\nDescription:  Extract only 33 rows from MySQL sub-partition into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 33 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_292000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Partition_Limit22_to_CSV_Default': ('MYSQL', 'CSV', ('', '', 'Extract only 22 rows from MySQL partition into CSV Default location.'), 'Use case name: MYSQL_Partition_Limit22_to_CSV_Default\nDescription:  Extract only 22 rows from MySQL partition into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 22 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_094000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_ShardedTable_to_CSV_Default': ('MYSQL', 'CSV', ('', 'Break input table into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Extract MySQL table into CSV Default location.'), 'Use case name: MYSQL_ShardedTable_to_CSV_Default\nDescription:  Break input table into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  Extract MySQL table into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_476000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Subpartition_Limit33_to_CSV_File': ('MYSQL', 'CSV', ('', '', 'Extract only 33 rows from MySQL sub-partition into CSV File location.'), 'Use case name: MYSQL_Subpartition_Limit33_to_CSV_File\nDescription:  Extract only 33 rows from MySQL sub-partition into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 33 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_345000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_Subpartition_Limit33.csv'), 'MYSQL_Partition_to_CSV_File': ('MYSQL', 'CSV', ('', '', 'Extract MySQL partition into CSV File location.'), 'Use case name: MYSQL_Partition_to_CSV_File\nDescription:  Extract MySQL partition into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_792000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_Partition.csv'), 'MYSQL_ShardedTable_to_CSV_Dir': ('MYSQL', 'CSV', ('', 'Break input table into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Extract MySQL table into CSV Dir location.'), 'Use case name: MYSQL_ShardedTable_to_CSV_Dir\nDescription:  Break input table into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run extract process on each shard in thread pool (-o[--pool_size] -1).\n\t  Extract MySQL table into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105950_408000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_Table_to_CSV_File': ('MYSQL', 'CSV', ('', '', 'Extract MySQL table into CSV File location.'), 'Use case name: MYSQL_Table_to_CSV_File\nDescription:  Extract MySQL table into CSV File location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -g[--to_file] is "To CSV file."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_980000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -g C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT\\testMYSQL_Table.csv'), 'MYSQL_Subpartition_to_CSV_Dir': ('MYSQL', 'CSV', ('', '', 'Extract MySQL sub-partition into CSV Dir location.'), 'Use case name: MYSQL_Subpartition_to_CSV_Dir\nDescription:  Extract MySQL sub-partition into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105948_791000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT'), 'MYSQL_Table_to_CSV_Default': ('MYSQL', 'CSV', ('', '', 'Extract MySQL table into CSV Default location.'), 'Use case name: MYSQL_Table_to_CSV_Default\nDescription:  Extract MySQL table into CSV Default location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_923000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin"'), 'MYSQL_TimezoneQueryFile_to_CSV_Dir': ('MYSQL', 'CSV', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Extract MySQL query results into CSV Dir location.'), 'Use case name: MYSQL_TimezoneQueryFile_to_CSV_Dir\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Extract MySQL query results into CSV Dir location.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -D[--to_dir] is "To directory."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-CSV ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105949_111000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -D C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\CSV_OUT')}, 'CSV_to_MySQL': {'CSV_ShardedFile_to_MYSQL_Table': ('CSV', 'MYSQL', ('', 'Break input CSV file into 3 logical partitions (-r[--num_of_shards] 3) \n\t  and run load process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_ShardedFile_to_MYSQL_Table\nDescription:  Break input CSV file into 3 logical partitions (-r[--num_of_shards] 3) \n\t  and run load process on each shard in thread pool (-o[--pool_size] -1).\n\t  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105946_820000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_ShardedFileSkip1_to_MYSQL_Table': ('CSV', 'MYSQL', ('', 'Break input CSV file into 3 logical partitions (-r[--num_of_shards] 3) \n\t  and run load process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Skip 1 rows and load CSV file into MySQL Table table.'), 'Use case name: CSV_ShardedFileSkip1_to_MYSQL_Table\nDescription:  Break input CSV file into 3 logical partitions (-r[--num_of_shards] 3) \n\t  and run load process on each shard in thread pool (-o[--pool_size] -1).\n\t  Skip 1 rows and load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -y[--shard_size_kb] is "Shard size in KBytes (to partition file and to estimate number of lines in input CSV file)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_058000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data ^\n  -y 1 ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_File_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_File_to_MYSQL_Table\nDescription:  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_663000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_File_TableNamedFile_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_File_TableNamedFile_to_MYSQL_Table\nDescription:  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105946_942000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i ".\\test\\v101\\data\\SCOTT.Timestamp_test_to.data" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_ShardedDirSkip1_to_MYSQL_Table': ('CSV', 'MYSQL', ('Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  ', 'Break input CSV files into logical partitions (shards) and run load \n\t  process on each shard in thread pool (-o[--pool_size] -1)\n\t  ', 'Skip 1 rows and load CSV file into MySQL Table table.'), 'Use case name: CSV_ShardedDirSkip1_to_MYSQL_Table\nDescription:  Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  Break input CSV files into logical partitions (shards) and run load \n\t  process on each shard in thread pool (-o[--pool_size] -1)\n\t  Skip 1 rows and load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -I[--input_dirs] is "Input CSV directory."\n  -y[--shard_size_kb] is "Shard size in KBytes (to partition file and to estimate number of lines in input CSV file)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_599000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -I .\\test\\v101\\data\\mysql_data_dir ^\n  -y 1 ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_DirsSkip1_to_MYSQL_Table': ('CSV', 'MYSQL', ('Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  ', '', 'Skip 1 rows and load CSV file into MySQL Table table.'), 'Use case name: CSV_DirsSkip1_to_MYSQL_Table\nDescription:  Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  Skip 1 rows and load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -I[--input_dirs] is "Input CSV directory."\n  -y[--shard_size_kb] is "Shard size in KBytes (to partition file and to estimate number of lines in input CSV file)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_959000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -I .\\test\\v101\\data\\mysql_data_dir ^\n  -y 1 ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_Files_TableNamedFile_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_Files_TableNamedFile_to_MYSQL_Table\nDescription:  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_342000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i ".\\test\\v101\\data\\SCOTT.Timestamp_test_to.data",".\\test\\v101\\data\\SCOTT.Timestamp_test_to_2.data" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_FileSkip1_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Skip 1 rows and load CSV file into MySQL Table table.'), 'Use case name: CSV_FileSkip1_to_MYSQL_Table\nDescription:  Skip 1 rows and load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -y[--shard_size_kb] is "Shard size in KBytes (to partition file and to estimate number of lines in input CSV file)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_543000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data ^\n  -y 1 ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_File_Limit10_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Load only 10 rows from CSV file into MySQL Table table.'), 'Use case name: CSV_File_Limit10_to_MYSQL_Table\nDescription:  Load only 10 rows from CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_005000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_Dir_to_MYSQL_Table': ('CSV', 'MYSQL', ('Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  ', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_Dir_to_MYSQL_Table\nDescription:  Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -I[--input_dirs] is "Input CSV directory."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_421000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -I .\\test\\v101\\data\\mysql_data_dir ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_Dirs_Limit10_to_MYSQL_Table': ('CSV', 'MYSQL', ('Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  ', '', 'Load only 10 rows from CSV file into MySQL Table table.'), 'Use case name: CSV_Dirs_Limit10_to_MYSQL_Table\nDescription:  Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  Load only 10 rows from CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -I[--input_dirs] is "Input CSV directory."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105946_873000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -I .\\test\\v101\\data\\mysql_data_dir ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_MongoFile_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_MongoFile_to_MYSQL_Table\nDescription:  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_790000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\oracle_shard_0_mongo.csv ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_Dirs_to_MYSQL_Table': ('CSV', 'MYSQL', ('Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  ', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_Dirs_to_MYSQL_Table\nDescription:  Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -I[--input_dirs] is "Input CSV directory."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105946_757000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -I .\\test\\v101\\data\\mysql_data_dir ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_ShardedDir_Limit10_to_MYSQL_Table': ('CSV', 'MYSQL', ('Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  ', 'Break input CSV files into logical partitions (shards) and run load \n\t  process on each shard in thread pool (-o[--pool_size] -1)\n\t  ', 'Load only 10 rows from CSV file into MySQL Table table.'), 'Use case name: CSV_ShardedDir_Limit10_to_MYSQL_Table\nDescription:  Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  Break input CSV files into logical partitions (shards) and run load \n\t  process on each shard in thread pool (-o[--pool_size] -1)\n\t  Load only 10 rows from CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -I[--input_dirs] is "Input CSV directory."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_489000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -I .\\test\\v101\\data\\mysql_data_dir ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_ShardedFile_Limit10_to_MYSQL_Table': ('CSV', 'MYSQL', ('', 'Break input CSV file into 3 logical partitions (-r[--num_of_shards] 3) \n\t  and run load process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Load only 10 rows from CSV file into MySQL Table table.'), 'Use case name: CSV_ShardedFile_Limit10_to_MYSQL_Table\nDescription:  Break input CSV file into 3 logical partitions (-r[--num_of_shards] 3) \n\t  and run load process on each shard in thread pool (-o[--pool_size] -1).\n\t  Load only 10 rows from CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_843000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_TimezoneFile_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_TimezoneFile_to_MYSQL_Table\nDescription:  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_274000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_DateFiles_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_DateFiles_to_MYSQL_Table\nDescription:  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_726000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data,.\\test\\v101\\data\\mysql_shard_0.data ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_TimestampFile_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_TimestampFile_to_MYSQL_Table\nDescription:  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_174000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_DateFile_to_MYSQL_Table': ('CSV', 'MYSQL', ('', '', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_DateFile_to_MYSQL_Table\nDescription:  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -i[--input_files] is "Input CSV file(s)."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_121000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -i .\\test\\v101\\data\\mysql_shard_0.data ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'CSV_ShardedDir_to_MYSQL_Table': ('CSV', 'MYSQL', ('Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  ', 'Break input CSV files into logical partitions (shards) and run load \n\t  process on each shard in thread pool (-o[--pool_size] -1)\n\t  ', 'Load CSV file into MySQL Table table.'), 'Use case name: CSV_ShardedDir_to_MYSQL_Table\nDescription:  Read each CSV file from a directory ".\\test\\v101\\data\\mysql_data_dir".\n\t  Break input CSV files into logical partitions (shards) and run load \n\t  process on each shard in thread pool (-o[--pool_size] -1)\n\t  Load CSV file into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -dbg[--debug_level] is "QC Debug level."\n  -I[--input_dirs] is "Input CSV directory."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w CSV-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -B qc_job ^\n  -Y 20160603_105947_906000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -dbg 1 ^\n  -I .\\test\\v101\\data\\mysql_data_dir ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"')}, 'MySQL_to_MySQL': {'MYSQL_ShardedPartition_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', 'Break input sharded partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run copy process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Copy MySQL sharded partition into MySQL Table table.'), 'Use case name: MYSQL_ShardedPartition_to_MYSQL_Table\nDescription:  Break input sharded partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run copy process on each shard in thread pool (-o[--pool_size] -1).\n\t  Copy MySQL sharded partition into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105945_756000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_ShardedQuery_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', 'Break input query results into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run copy process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Copy MySQL query results into MySQL Table table.'), 'Use case name: MYSQL_ShardedQuery_to_MYSQL_Table\nDescription:  Break input query results into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run copy process on each shard in thread pool (-o[--pool_size] -1).\n\t  Copy MySQL query results into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105946_457000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_QueryDir_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  ', '', 'Copy MySQL query results into MySQL Table table.'), 'Use case name: MYSQL_QueryDir_to_MYSQL_Table\nDescription:  Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  Copy MySQL query results into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -Q[--query_sql_dir] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105946_140000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -Q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_QueryFile_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Copy MySQL query results into MySQL Table table.'), 'Use case name: MYSQL_QueryFile_to_MYSQL_Table\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Copy MySQL query results into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105946_672000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_ShardedSubpartition_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', 'Break input sharded sub-partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run copy process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Copy MySQL sharded sub-partition into MySQL Table table.'), 'Use case name: MYSQL_ShardedSubpartition_to_MYSQL_Table\nDescription:  Break input sharded sub-partition into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run copy process on each shard in thread pool (-o[--pool_size] -1).\n\t  Copy MySQL sharded sub-partition into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105945_922000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_QueryDir_Limit333_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  ', '', 'Copy only 333 rows from MySQL query results into MySQL Table table.'), 'Use case name: MYSQL_QueryDir_Limit333_to_MYSQL_Table\nDescription:  Read each SQL query file from a directory "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql".\n\t  Copy only 333 rows from MySQL query results into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -Q[--query_sql_dir] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 333 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105945_656000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -Q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\query_dir_mysql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Subpartition_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', '', 'Copy MySQL sub-partition into MySQL Table table.'), 'Use case name: MYSQL_Subpartition_to_MYSQL_Table\nDescription:  Copy MySQL sub-partition into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105945_818000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Table_Limit1000_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', '', 'Copy only 1000 rows from MySQL table into MySQL Table table.'), 'Use case name: MYSQL_Table_Limit1000_to_MYSQL_Table\nDescription:  Copy only 1000 rows from MySQL table into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 1000 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105946_056000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_ShardedTable_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', 'Break input table into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run copy process on each shard in thread pool (-o[--pool_size] -1).\n\t  ', 'Copy MySQL table into MySQL Table table.'), 'Use case name: MYSQL_ShardedTable_to_MYSQL_Table\nDescription:  Break input table into 3 logical shards (-r[--num_of_shards] 3) \n\t  and run copy process on each shard in thread pool (-o[--pool_size] -1).\n\t  Copy MySQL table into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 3 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105946_609000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_QueryFile_Limit100_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Copy only 100 rows from MySQL query results into MySQL Table table.'), 'Use case name: MYSQL_QueryFile_Limit100_to_MYSQL_Table\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Copy only 100 rows from MySQL query results into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 100 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105945_587000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Table_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', '', 'Copy MySQL table into MySQL Table table.'), 'Use case name: MYSQL_Table_to_MYSQL_Table\nDescription:  Copy MySQL table into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105946_388000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Table_KeepSpoolFile_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', '', 'Copy MySQL table into MySQL Table table.'), 'Use case name: MYSQL_Table_KeepSpoolFile_to_MYSQL_Table\nDescription:  Copy MySQL table into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105946_237000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Timestamp_test_from ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Partition_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', '', 'Copy MySQL partition into MySQL Table table.'), 'Use case name: MYSQL_Partition_to_MYSQL_Table\nDescription:  Copy MySQL partition into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105946_319000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Subpartition_Limit33_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', '', 'Copy only 33 rows from MySQL sub-partition into MySQL Table table.'), 'Use case name: MYSQL_Subpartition_Limit33_to_MYSQL_Table\nDescription:  Copy only 33 rows from MySQL sub-partition into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -S[--from_sub_partition] is "From sub-partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 33 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105946_539000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Sub_Partitioned_test_from ^\n  -S subpart200 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_TimezoneQueryFile_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  ', '', 'Copy MySQL query results into MySQL Table table.'), 'Use case name: MYSQL_TimezoneQueryFile_to_MYSQL_Table\nDescription:  Read SQL from a query file "C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql".\n\t  Copy MySQL query results into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -q[--query_sql_file] is "Input file with MySQL query sql."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 10 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105945_987000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -q C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc32\\test\\v101\\query\\mysql_query.sql ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"'), 'MYSQL_Partition_Limit22_to_MYSQL_Table': ('MYSQL', 'MYSQL', ('', '', 'Copy only 22 rows from MySQL partition into MySQL Table table.'), 'Use case name: MYSQL_Partition_Limit22_to_MYSQL_Table\nDescription:  Copy only 22 rows from MySQL partition into MySQL Table table.\nArguments:\n  -w[--copy_vector] is "Data copy direction."\n  -ps[--pool_size] is "Pool size."\n  -r[--num_of_shards] is "Number of shards."\n  -t[--field_term] is "Field terminator."\n  -l[--lame_duck] is "Limit rows (lame duck run)."\n  -K[--keep_data_file] is "Keep data dump."\n  -M[--log_dir] is "Log destination."\n  -F[--default_spool_dir] is "Default data dump dir (default_spool_dir/job_name/timestamp)."\n  -B[--job_name] is "Job name (log_dir/job_name)."\n  -Y[--time_stamp] is "Timestamp (log_dir/job_name/timestamp)."\n  -5[--host_map] is "Host-to-shard map."\n  -6[--spool_type] is "Spool file type (CSV or JSON)."\n  -dbg[--debug_level] is "QC Debug level."\n  -c[--from_table] is "From table."\n  -P[--from_partition] is "From partition."\n  -j[--from_user] is "MySQL source user."\n  -x[--from_passwd] is "MySQL source user password."\n  -b[--from_db_name] is "MySQL source database."\n  -n[--from_db_server] is "MySQL source instance name."\n  -z[--source_client_home] is "Path to MySQL client home."\n  -u[--to_user] is "Target MySQL db user."\n  -p[--to_passwd] is "Target db user password."\n  -d[--to_db_name] is "Target database."\n  -s[--to_db_server] is "Target db instance name."\n  -a[--to_table] is "Target table."\n  -Z[--target_client_home] is "Path to mysql client home."\t\nExample: \n  echo y|C:\\Users\\alex_buz\\Documents\\GitHub\\DataBuddy\\sources\\qc_dist_32\\20160603_105945\\qc32\\qc32.exe ^\n  -w MYSQL-MYSQL ^\n  -ps 1 ^\n  -r 1 ^\n  -t "|" ^\n  -l 22 ^\n  -K 1 ^\n  -M C:\\Temp\\qc_log ^\n  -F C:\\tmp\\TEST_default_spool ^\n  -B qc_job ^\n  -Y 20160603_105945_518000 ^\n  -5 ".\\config\\host_map\\host_map.py" ^\n  -6 csv ^\n  -dbg 1 ^\n  -c TEST.Partitioned_test_from ^\n  -P rx2015 ^\n  -j "alex" ^\n  -x "mysql_pwd" ^\n  -b "test" ^\n  -n "localhost" ^\n  -z "C:\\Temp\\mysql\\bin" ^\n  -u alex ^\n  -p mysql_pwd ^\n  -d test ^\n  -s localhost ^\n  -a Timestamp_test_to ^\n  -Z "C:\\Temp\\mysql\\bin"')}}
